{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f67dabd",
   "metadata": {},
   "source": [
    "# Pitcher WAR/162 Projection – Multi-Year t → t+1 (Optimized for Pitchers)\n",
    "\n",
    "This notebook predicts **next-year WAR per 162 games (WAR/162)** for pitchers from\n",
    "**current-year features** using the multi-year dataset:\n",
    "\n",
    "`data/Pitchers_2015-2025_byYear_retry.csv`\n",
    "\n",
    "Design decisions are tuned for **pitchers**:\n",
    "\n",
    "- Build a **year t → year t+1** panel for all seasons.\n",
    "- Restrict to pitchers with **meaningful workloads** in both seasons to reduce noise\n",
    "  (default: `IP_t ≥ 30` and `IP_{t+1} ≥ 30`).\n",
    "- Drop features that are missing in >60% of rows (e.g., sparse Statcast / bat-tracking fields).\n",
    "- Two feature configurations:\n",
    "  - `skill_only`: excludes current-year WAR/WAR_per_162 (pure skill & context).\n",
    "  - `with_prev_war`: includes current-year WAR and WAR_per_162 (realistic projection setup).\n",
    "- Models:\n",
    "  - **ElasticNetCV** (regularized linear regression).\n",
    "  - **MLPRegressor** (simple NN for tabular data).\n",
    "- Train on all transitions with `Season ≤ 2023` and hold out `Season == 2024` (predicting 2025).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ed08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (7, 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eafbe48",
   "metadata": {},
   "source": [
    "## 1. Load pitcher-by-year dataset and compute WAR_per_162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2678c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/Pitchers_2015-2025_byYear_retry.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "print('Shape:', df.shape)\n",
    "print('Columns (first 25):', list(df.columns)[:25])\n",
    "print('\\nSeason value counts:')\n",
    "print(df['Season'].value_counts().sort_index())\n",
    "\n",
    "df = df.copy()\n",
    "df['WAR_per_162'] = (df['WAR'] / df['G'].replace(0, np.nan)) * 162\n",
    "print('\\nSample WAR_per_162 rows:')\n",
    "print(df[['mlbID', 'Season', 'G', 'WAR', 'WAR_per_162']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944b51c",
   "metadata": {},
   "source": [
    "## 2. Build year t → year t+1 panel\n",
    "\n",
    "We construct pairs where:\n",
    "- Features come from season `t`.\n",
    "- Target (`WAR_per_162_next`) comes from season `t+1` for the same pitcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcaef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_season, max_season = df['Season'].min(), df['Season'].max()\n",
    "print('Seasons span:', min_season, 'to', max_season)\n",
    "\n",
    "# t side: all but last season\n",
    "df_t = df[df['Season'] < max_season].copy()\n",
    "\n",
    "# t+1 target side\n",
    "df_target = df[['mlbID', 'Season', 'WAR_per_162', 'IP']].copy()\n",
    "df_target = df_target.rename(columns={\n",
    "    'Season': 'Season_next',\n",
    "    'WAR_per_162': 'WAR_per_162_next',\n",
    "    'IP': 'IP_next',\n",
    "})\n",
    "df_target['Season'] = df_target['Season_next'] - 1\n",
    "\n",
    "panel = df_t.merge(\n",
    "    df_target[['mlbID', 'Season', 'Season_next', 'WAR_per_162_next', 'IP_next']],\n",
    "    on=['mlbID', 'Season'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print('Panel shape (t -> t+1 pairs):', panel.shape)\n",
    "print('\\nPairs by t (Season):')\n",
    "print(panel['Season'].value_counts().sort_index())\n",
    "print('\\nPairs by t+1 (Season_next):')\n",
    "print(panel['Season_next'].value_counts().sort_index())\n",
    "\n",
    "# Drop missing targets\n",
    "panel = panel.dropna(subset=['WAR_per_162', 'WAR_per_162_next'])\n",
    "print('\\nPanel after dropping missing WAR_per_162:', panel.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c3069",
   "metadata": {},
   "source": [
    "## 3. Filter to pitchers with meaningful workloads\n",
    "\n",
    "Next-year WAR is extremely noisy for tiny samples and cups of coffee.\n",
    "We restrict to pitchers who have **at least a minimum IP** in both t and t+1.\n",
    "\n",
    "Default:\n",
    "- `MIN_IP_T = 30`\n",
    "- `MIN_IP_TP1 = 30`\n",
    "\n",
    "You can relax or tighten these thresholds depending on your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5257d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_IP_T = 30.0     # minimum IP in season t\n",
    "MIN_IP_TP1 = 30.0   # minimum IP in season t+1\n",
    "\n",
    "workload_mask = (panel['IP'] >= MIN_IP_T) & (panel['IP_next'] >= MIN_IP_TP1)\n",
    "panel_w = panel[workload_mask].copy()\n",
    "\n",
    "print('Panel (workload-filtered) shape:', panel_w.shape)\n",
    "print('\\nWorkload-filtered pairs by t (Season):')\n",
    "print(panel_w['Season'].value_counts().sort_index())\n",
    "\n",
    "# Correlation of WAR_per_162_t vs WAR_per_162_{t+1} after filter (sanity upper bound)\n",
    "corr = panel_w[['WAR_per_162', 'WAR_per_162_next']].corr().iloc[0, 1]\n",
    "print(f\"Correlation WAR_per_162_t vs WAR_per_162_t+1 (filtered): {corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc05fe6e",
   "metadata": {},
   "source": [
    "## 4. Define numeric feature columns and drop highly missing features\n",
    "\n",
    "We start from all numeric columns, excluding:\n",
    "- Identifiers and year markers: `mlbID`, `Season`, `Season_next`.\n",
    "- Target: `WAR_per_162_next`.\n",
    "\n",
    "Then we drop any feature with >60% missing values in the workload-filtered panel\n",
    "to avoid letting extremely sparse Statcast/bat-tracking features dominate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric columns\n",
    "numeric_cols = panel_w.select_dtypes(include=[np.number]).columns.tolist()\n",
    "exclude_base = {'mlbID', 'Season', 'Season_next', 'WAR_per_162_next'}\n",
    "base_feature_cols = [c for c in numeric_cols if c not in exclude_base]\n",
    "print('Initial numeric feature count (incl current-year WAR):', len(base_feature_cols))\n",
    "\n",
    "# Drop features with >60% missingness\n",
    "missing_frac = panel_w[base_feature_cols].isna().mean()\n",
    "too_sparse = missing_frac[missing_frac > 0.60].index.tolist()\n",
    "print('Features with >60% missingness:', len(too_sparse))\n",
    "\n",
    "base_feature_cols = [c for c in base_feature_cols if c not in too_sparse]\n",
    "print('Final feature count after dropping sparse columns:', len(base_feature_cols))\n",
    "print('Example features:', base_feature_cols[:25])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758f0da",
   "metadata": {},
   "source": [
    "## 5. Train/test split (multi-year)\n",
    "\n",
    "- **Train**: all transitions with `Season ≤ 2023` (predicting up through 2024).\n",
    "- **Test**: transitions with `Season == 2024` (predicting 2025)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_panel = panel_w[panel_w['Season'] <= 2023].copy()\n",
    "test_panel = panel_w[panel_w['Season'] == 2024].copy()\n",
    "\n",
    "print('Train panel shape:', train_panel.shape)\n",
    "print('Test panel shape :', test_panel.shape)\n",
    "\n",
    "y_train_full = train_panel['WAR_per_162_next'].values\n",
    "y_test_full = test_panel['WAR_per_162_next'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb01793",
   "metadata": {},
   "source": [
    "## 6. Helper: run one experiment (feature_mode + models)\n",
    "\n",
    "We support two feature modes:\n",
    "- `skill_only`: drop current-year `WAR` and `WAR_per_162` from features.\n",
    "- `with_prev_war`: keep them as features (captures persistence in true talent & usage).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46814cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(train_panel, test_panel, base_feature_cols, feature_mode='skill_only', random_state=42):\n",
    "    # Start from base features\n",
    "    feature_cols = base_feature_cols.copy()\n",
    "\n",
    "    if feature_mode == 'skill_only':\n",
    "        feature_cols = [c for c in feature_cols if c not in {'WAR', 'WAR_per_162'}]\n",
    "    elif feature_mode == 'with_prev_war':\n",
    "        # Keep WAR and WAR_per_162 as features\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f'Unknown feature_mode: {feature_mode}')\n",
    "\n",
    "    # Drop any rows with missing target just in case\n",
    "    train_subset = train_panel.dropna(subset=['WAR_per_162_next']).copy()\n",
    "    test_subset = test_panel.dropna(subset=['WAR_per_162_next']).copy()\n",
    "\n",
    "    X_train = train_subset[feature_cols].values\n",
    "    y_train = train_subset['WAR_per_162_next'].values\n",
    "    X_test = test_subset[feature_cols].values\n",
    "    y_test = test_subset['WAR_per_162_next'].values\n",
    "\n",
    "    # Impute + scale\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train_imp = imputer.fit_transform(X_train)\n",
    "    X_test_imp = imputer.transform(X_test)\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train_imp)\n",
    "    X_test_scaled = scaler.transform(X_test_imp)\n",
    "\n",
    "    # ElasticNetCV\n",
    "    elastic_cv = ElasticNetCV(\n",
    "        l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "        alphas=np.logspace(-3, 2, 20),\n",
    "        cv=5,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    elastic_cv.fit(X_train_scaled, y_train)\n",
    "    y_train_pred_lr = elastic_cv.predict(X_train_scaled)\n",
    "    y_test_pred_lr = elastic_cv.predict(X_test_scaled)\n",
    "\n",
    "    lr_train_rmse = mean_squared_error(y_train, y_train_pred_lr, squared=False)\n",
    "    lr_test_rmse = mean_squared_error(y_test, y_test_pred_lr, squared=False)\n",
    "    lr_train_r2 = r2_score(y_train, y_train_pred_lr)\n",
    "    lr_test_r2 = r2_score(y_test, y_test_pred_lr)\n",
    "\n",
    "    # Neural net\n",
    "    mlp = MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation='relu',\n",
    "        alpha=1e-4,\n",
    "        learning_rate_init=1e-3,\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "    y_train_pred_nn = mlp.predict(X_train_scaled)\n",
    "    y_test_pred_nn = mlp.predict(X_test_scaled)\n",
    "\n",
    "    nn_train_rmse = mean_squared_error(y_train, y_train_pred_nn, squared=False)\n",
    "    nn_test_rmse = mean_squared_error(y_test, y_test_pred_nn, squared=False)\n",
    "    nn_train_r2 = r2_score(y_train, y_train_pred_nn)\n",
    "    nn_test_r2 = r2_score(y_test, y_test_pred_nn)\n",
    "\n",
    "    return {\n",
    "        'feature_mode': feature_mode,\n",
    "        'n_train': len(y_train),\n",
    "        'n_test': len(y_test),\n",
    "        'feature_count': len(feature_cols),\n",
    "        'feature_names': feature_cols,\n",
    "        'lr_train_rmse': lr_train_rmse,\n",
    "        'lr_test_rmse': lr_test_rmse,\n",
    "        'lr_train_r2': lr_train_r2,\n",
    "        'lr_test_r2': lr_test_r2,\n",
    "        'nn_train_rmse': nn_train_rmse,\n",
    "        'nn_test_rmse': nn_test_rmse,\n",
    "        'nn_train_r2': nn_train_r2,\n",
    "        'nn_test_r2': nn_test_r2,\n",
    "        'elastic_model': elastic_cv,\n",
    "        'mlp_model': mlp,\n",
    "        'y_test': y_test,\n",
    "        'y_test_pred_lr': y_test_pred_lr,\n",
    "        'y_test_pred_nn': y_test_pred_nn,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0cec2",
   "metadata": {},
   "source": [
    "## 7. Run experiments for `skill_only` and `with_prev_war`\n",
    "\n",
    "We run both feature configurations and compare test RMSE / R²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30fd138",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_modes = ['skill_only', 'with_prev_war']\n",
    "all_results = []\n",
    "\n",
    "for fm in feature_modes:\n",
    "    print(f\"\\n=== Running experiment: feature_mode={fm} ===\")\n",
    "    res = run_experiment(train_panel, test_panel, base_feature_cols, feature_mode=fm, random_state=42)\n",
    "    all_results.append(res)\n",
    "\n",
    "summary_rows = []\n",
    "for r in all_results:\n",
    "    summary_rows.append({\n",
    "        'feature_mode': r['feature_mode'],\n",
    "        'n_train': r['n_train'],\n",
    "        'n_test': r['n_test'],\n",
    "        'feature_count': r['feature_count'],\n",
    "        'lr_test_rmse': r['lr_test_rmse'],\n",
    "        'lr_test_r2': r['lr_test_r2'],\n",
    "        'nn_test_rmse': r['nn_test_rmse'],\n",
    "        'nn_test_r2': r['nn_test_r2'],\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df = summary_df.sort_values(by='nn_test_r2', ascending=False)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba77dcf8",
   "metadata": {},
   "source": [
    "## 8. Inspect best configuration and its features\n",
    "\n",
    "We pick the configuration with the highest NN test R² and print its feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea79cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = int(np.argmax([r['nn_test_r2'] for r in all_results]))\n",
    "best_res = all_results[best_idx]\n",
    "\n",
    "print('Best config:')\n",
    "print(' feature_mode:', best_res['feature_mode'])\n",
    "print(' nn_test_r2  :', best_res['nn_test_r2'])\n",
    "print('\\nFeatures used (best model):')\n",
    "print(best_res['feature_names'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e033f2e",
   "metadata": {},
   "source": [
    "## 9. Diagnostics: predicted vs actual WAR/162 (test set) for best NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8780dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = best_res['y_test']\n",
    "y_pred = best_res['y_test_pred_nn']\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "lims = [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())]\n",
    "plt.plot(lims, lims, linestyle='--')\n",
    "plt.xlabel('Actual WAR/162 (t+1, test)')\n",
    "plt.ylabel('Predicted WAR/162 (t+1, test)')\n",
    "plt.title(f\"Predicted vs Actual WAR/162 – NN | {best_res['feature_mode']}\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
